import os, json, zipfile, argparse, torch, time
from pathlib import Path
from ultralytics import YOLO
from PIL import Image
from dotenv import load_dotenv
from datetime import datetime
import requests, colorsys
from math import ceil
from concurrent.futures import ThreadPoolExecutor
import pandas as pd
import csv
from itertools import cycle
from typing import Optional, Set, Iterable, List, Dict

# ====== ENV ======
env_path = Path(__file__).resolve().parent.parent / ".env"
load_dotenv(dotenv_path=env_path)
CVAT_URL = os.getenv("CVAT_URL_2")
TOKEN = os.getenv("TOKEN_2")
ORGANIZATIONS = [org.strip() for org in os.getenv("ORGANIZATIONS", "").split(",")]
ASSIGN_LOG_PATH = Path(f"./logs/assignments_log.csv")
ASSIGN_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)

# ====== Utils ======
def hsv_to_hex(h, s, v):
    r, g, b = colorsys.hsv_to_rgb(h, s, v)
    return '#{:02X}{:02X}{:02X}'.format(int(r * 255), int(g * 255), int(b * 255))

def _debug_http_error(prefix, res):
    print(f"[{prefix}] status={res.status_code}")
    try:
        print(f"[{prefix}] body.json=", res.json())
    except Exception:
        print(f"[{prefix}] body.text=", res.text)

def _safe_json(res):
    try:
        return res.json()
    except Exception:
        return {"_text": res.text}

# ====== CVAT API (organizations / headers / preflight) ======
def get_or_create_organization(name):
    """ì¡°ì§ ì¡°íšŒ/ìƒì„±. (í—¤ë”ì— org ë¯¸í¬í•¨)"""
    headers = {"Authorization": f"Token {TOKEN}", "Content-Type": "application/json"}
    res = requests.get(f"{CVAT_URL}/api/organizations", headers=headers)
    res.raise_for_status()
    for org in res.json().get("results", []):
        if org["slug"] == name or org["name"] == name:
            return org["id"], org["slug"]
    slug = name.lower().replace(" ", "-")
    res = requests.post(f"{CVAT_URL}/api/organizations", headers=headers, json={"name": name, "slug": slug})
    res.raise_for_status()
    return res.json()["id"], slug

def build_headers(org_slug):
    """ê³µí†µ í—¤ë”: ì¼ë¶€ ë°°í¬ì—ì„œ ì»¤ìŠ¤í…€ í—¤ë” ë“œë¡­ ë°©ì§€ë¥¼ ìœ„í•´ ì¿¼ë¦¬ìŠ¤íŠ¸ë§ë„ ë³‘í–‰ ì‚¬ìš©"""
    return {
        "Authorization": f"Token {TOKEN}",
        "Content-Type": "application/json",
        "X-Organization": org_slug
    }

def preflight_check(headers, org_slug):
    """ë™ì¼ ì»¨í…ìŠ¤íŠ¸ë¡œ /api/tasks ì ‘ê·¼ì´ í—ˆìš©ë˜ëŠ”ì§€ ì‚¬ì „ í™•ì¸"""
    url = f"{CVAT_URL}/api/tasks?org={org_slug}"
    try:
        res = requests.get(url, headers=headers)
        if res.status_code == 200:
            print("âœ… Preflight OK: /api/tasks GET authorized with org context")
            return True
        else:
            _debug_http_error("Preflight /api/tasks", res)
            return False
    except Exception as e:
        print("âŒ Preflight exception:", e)
        return False

def _normalize_and_dedupe_labels(labels):
    """ë¼ë²¨ ì •ê·œí™”+ì¤‘ë³µ ì œê±°(ìˆœì„œ ë³´ì¡´) ë° ê¸°ë³¸ attributes í¬í•¨"""
    seen = set()
    uniq = []
    for raw in labels:
        lbl = str(raw).strip()
        if not lbl:
            continue
        if lbl in seen:
            print(f"[WARN] Duplicate label skipped: '{lbl}'")
            continue
        seen.add(lbl)
        uniq.append(lbl)
    if not uniq:
        raise ValueError("No valid labels after normalization. Check --labels arguments.")
    label_defs = []
    for i, lbl in enumerate(uniq):
        label_defs.append({
            "name": lbl,
            "color": hsv_to_hex(i / max(1, len(uniq)), 0.7, 0.95),
            "attributes": []  # ì¼ë¶€ ë²„ì „ì—ì„œ í•„ìˆ˜
        })
    return label_defs, uniq

def create_project(name, labels, headers, org_slug):
    """í”„ë¡œì íŠ¸ ìƒì„±(ì¤‘ë³µ ë¼ë²¨ ë°©ì§€, org ì¿¼ë¦¬ ë³‘í–‰)"""
    label_defs, uniq_labels = _normalize_and_dedupe_labels(labels)
    base = f"{CVAT_URL}/api/projects"
    url_candidates = [f"{base}/?org={org_slug}", f"{base}?org={org_slug}", f"{base}/", base]
    last_err = None
    for url in url_candidates:
        res = requests.post(url, headers=headers, json={"name": name, "labels": label_defs})
        if res.status_code >= 400:
            _debug_http_error(f"Project create POST {url}", res)
        try:
            res.raise_for_status()
            pid = res.json()["id"]
            print(f"âœ… Project created via {url} â†’ id={pid}, labels={uniq_labels}")
            return pid
        except requests.HTTPError as e:
            last_err = e
    raise last_err

def create_task_with_zip(name, project_id, zip_path, headers, org_slug):
    """Task ìƒì„± + ZIP ì—…ë¡œë“œ. org ì¿¼ë¦¬ ë³‘í–‰ ë° íŠ¸ë ˆì¼ë§ ìŠ¬ë˜ì‹œ í˜¸í™˜."""
    base = f"{CVAT_URL}/api/tasks"
    url_candidates = [f"{base}/?org={org_slug}", f"{base}?org={org_slug}", f"{base}/", base]
    payload = {
        "name": name,
        "project_id": project_id,
        "image_quality": 100,
        "segment_size": 100
    }
    task_id = None
    last_err = None
    for url in url_candidates:
        res = requests.post(url, headers=headers, json=payload)
        if res.status_code >= 400:
            _debug_http_error(f"Task create POST {url}", res)
        try:
            res.raise_for_status()
            task_id = res.json()["id"]
            print(f"âœ… Task created via {url} â†’ id={task_id}")
            break
        except requests.HTTPError as e:
            last_err = e
            continue
    if task_id is None:
        raise last_err

    # ---- ë°ì´í„° ì—…ë¡œë“œ (files ì—…ë¡œë“œ ì‹œ Content-Type ì œê±° í•„ìˆ˜) ----
    upload_headers = headers.copy()
    upload_headers.pop("Content-Type", None)
    with open(zip_path, "rb") as zip_file:
        files = {"client_files[0]": (os.path.basename(zip_path), zip_file, "application/zip")}
        data = {
            "image_quality": 100,
            "use_zip_chunks": "false",
            "use_cache": "false",
            "sorting_method": "lexicographical",
            "upload_format": "zip"
        }
        data_url = f"{CVAT_URL}/api/tasks/{task_id}/data?org={org_slug}"
        res = requests.post(data_url, headers=upload_headers, files=files, data=data)
        if res.status_code >= 400:
            _debug_http_error("Task data upload", res)
        res.raise_for_status()
        print(f"ğŸ“¦ Uploaded ZIP to task {task_id}")

    return task_id

def wait_until_task_ready(task_id, headers, org_slug, timeout=120):
    """í”„ë ˆì„ ì¸ë±ì‹± ì™„ë£Œê¹Œì§€ ëŒ€ê¸° (size>0)"""
    start = time.time()
    while time.time() - start < timeout:
        res = requests.get(f"{CVAT_URL}/api/tasks/{task_id}?org={org_slug}", headers=headers)
        if res.status_code != 200:
            print(f"âŒ Task ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {res.status_code}")
            break
        task_info = res.json()
        if task_info.get("size", 0) > 0:
            return True
        time.sleep(2)
    return False

def upload_annotations(task_id, json_path, headers, org_slug):
    """COCO 1.0 ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ (ì •í™•í•œ org ì „ë‹¬)"""
    print(f"â³ ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ ì‹œì‘: Task ID {task_id}")
    upload_headers = headers.copy()
    upload_headers.pop("Content-Type", None)
    with open(json_path, "rb") as jf:
        files = {"annotation_file": (json_path.name, jf, "application/json")}
        url = f"{CVAT_URL}/api/tasks/{task_id}/annotations"
        params = {
            "org": org_slug,
            "format": "COCO 1.0",
            "filename": json_path.name,
            "conv_mask_to_poly": "true"
        }
        res = requests.put(url, headers=upload_headers, files=files, params=params)

    if res.status_code in [200, 202]:
        print(f"âœ… ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ ì„±ê³µ: Task {task_id}")
    else:
        print(f"âŒ ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {res.status_code}")
        print(res.text)
        return False
    return True

def refresh_and_check_counts(task_id, headers, org_slug):
    """ì—…ë¡œë“œ ì§í›„ ì„œë²„ ì¸¡ ìš”ì•½ê°’ì„ ê°±ì‹ /ì¡°íšŒ."""
    try:
        url_reload = f"{CVAT_URL}/api/tasks/{task_id}/annotations?action=reload&org={org_slug}"
        r = requests.post(url_reload, headers=headers)
        print("ğŸ”„ annotations reload:", r.status_code)
    except Exception as e:
        print("reload skip:", e)

    time.sleep(1.0)
    meta = requests.get(f"{CVAT_URL}/api/tasks/{task_id}?org={org_slug}", headers=headers)
    if meta.status_code == 200:
        j = meta.json()
        print(f"ğŸ§¾ Task meta: size={j.get('size')} | segments={j.get('segments')}")
    else:
        _debug_http_error("Task meta refresh", meta)

def get_jobs(task_id, headers, org_slug):
    res = requests.get(f"{CVAT_URL}/api/jobs?task_id={task_id}&org={org_slug}", headers=headers)
    res.raise_for_status()
    return res.json().get("results", [])

def get_user_id(username: str, headers: dict, org_slug: str, page_size: int = 100):
    """íŠ¹ì • username ì— í•´ë‹¹í•˜ëŠ” user.id ë°˜í™˜ (ë í˜ì´ì§€ê¹Œì§€ íƒìƒ‰)."""
    url = f"{CVAT_URL}/api/users?org={org_slug}&page_size={page_size}"
    while url:
        res = requests.get(url, headers=headers)
        res.raise_for_status()
        data = res.json()
        for user in data.get("results", []) or []:
            if user.get("username") == username:
                return user.get("id")
        url = data.get("next")
    return None

# ====== memberships í˜ì´ì§€ë„¤ì´ì…˜ & role=worker í•„í„° ======
def _iter_paginated(url: str, headers: dict):
    """CVAT APIì˜ í‘œì¤€ í˜ì´ì§€ë„¤ì´ì…˜(next ë§í¬)ì„ ë”°ë¼ê°€ë©° resultsë¥¼ yield."""
    while url:
        res = requests.get(url, headers=headers)
        res.raise_for_status()
        data = res.json() or {}
        for item in data.get("results", []) or []:
            yield item
        url = data.get("next")

def get_all_memberships(headers: dict, org_slug: str, page_size: int = 100):
    """ì¡°ì§ì˜ ëª¨ë“  membershipì„ ë í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘."""
    start_url = f"{CVAT_URL}/api/memberships?org={org_slug}&page_size={page_size}"
    return list(_iter_paginated(start_url, headers))

def get_worker_usernames(headers: dict, org_slug: str, page_size: int = 100) -> Set[str]:
    """
    ì¡°ì§ì—ì„œ roleì´ 'worker'ì¸ ì‚¬ìš©ìë“¤ì˜ username ì§‘í•©ì„ ë°˜í™˜.
    memberships í•­ëª© ì˜ˆì‹œ: { "user": {...}, "role": "worker", ... }
    """
    workers = set()
    for m in get_all_memberships(headers, org_slug, page_size=page_size):
        role = (m or {}).get("role")
        u = (m or {}).get("user") or {}
        uname = u.get("username")
        if role == "worker" and uname:
            workers.add(uname)
    return workers

def filter_assignees_by_role_and_exclude(
    candidates: Iterable[str],
    worker_usernames: Set[str],
    exclude_users: Optional[Set[str]] = None,
) -> List[str]:
    """
    (candidates âˆ© workers) âˆ’ exclude_users ìˆœì„œ ìœ ì§€ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    - ê°™ì€ username ì¤‘ë³µì€ 1íšŒë§Œ ì‚¬ìš©
    """
    exclude_users = exclude_users or set()
    seen = set()
    filtered = []
    for a in candidates:
        if a in worker_usernames and a not in exclude_users and a not in seen:
            filtered.append(a)
            seen.add(a)
    return filtered

# ====== ë¼ìš´ë“œë¡œë¹ˆ by job ======
def assign_jobs_round_robin(
    jobs: List[dict],
    headers: dict,
    assignees: List[str],
    org_slug: str,
) -> Dict[str, int]:
    """
    ì—¬ëŸ¬ ëª…(assignees) ì‚¬ì´ì—ì„œ 'Job ë‹¨ìœ„'ë¡œ ê· ë“± ë¶„ë°°(ë¼ìš´ë“œë¡œë¹ˆ)
    ë°˜í™˜ê°’: {username: í• ë‹¹ëœ job ê°œìˆ˜}
    """
    if not assignees:
        print("â›” ë¼ìš´ë“œë¡œë¹ˆ ëŒ€ìƒìê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {}

    # username â†’ user_id ìºì‹œ
    id_cache: Dict[str, Optional[int]] = {}
    for name in assignees:
        uid = get_user_id(name, headers, org_slug)
        if not uid:
            print(f"âŒ ì‚¬ìš©ì '{name}'ì˜ user_idë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¶„ë°° ëŒ€ìƒì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.")
        id_cache[name] = uid
    assignees = [a for a in assignees if id_cache.get(a)]
    if not assignees:
        print("â›” ìœ íš¨í•œ user_idê°€ ìˆëŠ” ë¼ìš´ë“œë¡œë¹ˆ ëŒ€ìƒìê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {}

    cyc = cycle(assignees)
    assigned_count: Dict[str, int] = {a: 0 for a in assignees}

    # ì•ˆì •ì  ë¶„ë°°ë¥¼ ìœ„í•´ job id ê¸°ì¤€ ì •ë ¬
    jobs_sorted = sorted(jobs, key=lambda j: j.get("id", 0))

    for job in jobs_sorted:
        if job.get("assignee"):
            continue
        assignee = next(cyc)
        user_id = id_cache[assignee]
        try:
            res = requests.patch(
                f"{CVAT_URL}/api/jobs/{job['id']}?org={org_slug}",
                headers=headers,
                json={"assignee": user_id}
            )
            res.raise_for_status()
            assigned_count[assignee] += 1
            print(f"âœ… Job {job['id']} â†’ '{assignee}' (ëˆ„ì  {assigned_count[assignee]})")
        except requests.HTTPError as e:
            print(f"âš ï¸ í• ë‹¹ ì‹¤íŒ¨ (job {job['id']} â†’ {assignee}): {e.response.status_code} - {e.response.text}")

    return assigned_count

def get_user_display_name(username):
    return os.getenv(f"USERMAP_{username}", username)

def log_assignment(task_name, task_id, assignee_name, num_jobs, project_name, organization):
    """ì‚¬ìš©ìë³„ í• ë‹¹ ê²°ê³¼ë¥¼ í•œ ì¤„ì”© ê¸°ë¡"""
    now_dt = datetime.now()
    now_str = now_dt.strftime("%Y/%m/%d %H:%M")
    log_columns = ["timestamp","organization","project","task_name","task_id","assignee","num_jobs"]
    display_name = get_user_display_name(assignee_name)

    log_entry_dict = {
        "timestamp": now_str,
        "organization": organization,
        "project": project_name,
        "task_name": task_name,
        "task_id": task_id,
        "assignee": display_name,
        "num_jobs": num_jobs
    }

    if not ASSIGN_LOG_PATH.exists():
        with open(ASSIGN_LOG_PATH, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=log_columns)
            writer.writeheader()
            writer.writerow(log_entry_dict)
        return
    
    with open(ASSIGN_LOG_PATH, "r", newline="", encoding="utf-8") as f:
        reader = list(csv.DictReader(f))
        rows = []
        for r in reader:
            completed_row = {col: r.get(col, "") for col in log_columns}
            rows.append(completed_row)

    def parse_timestamp(ts):
        for fmt in ("%d/%m/%Y %H:%M", "%Y/%m/%d %H:%M"):
            try:
                return datetime.strptime(ts, fmt)
            except ValueError:
                continue
        print(f"[âš ï¸] ì˜ëª»ëœ ë‚ ì§œ í¬ë§·: {ts}")
        return datetime.min
    
    rows.append(log_entry_dict)
    rows.sort(key=lambda r: parse_timestamp(r["timestamp"]), reverse=True)

    with open(ASSIGN_LOG_PATH, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=log_columns)
        writer.writeheader()
        writer.writerows(rows)

# ====== YOLO / COCO ìƒì„± ======
def run_yolo_on_image(model, img_path, image_id, annotation_id_start):
    start = time.time()
    img = Image.open(img_path)
    width, height = img.size
    image_entry = {
        "id": image_id,
        "file_name": img_path.name,
        "width": width,
        "height": height
    }
    results = model(img_path)
    annotations = []
    aid = annotation_id_start
    for r in results:
        for i, box in enumerate(r.boxes):
            cls_id = int(r.boxes.cls[i].item())
            cls_name = r.names[cls_id]
            if cls_name != "person":
                continue
            x1, y1, x2, y2 = map(float, box.xyxy[0])
            bbox = [x1, y1, x2 - x1, y2 - y1]
            area = bbox[2] * bbox[3]
            annotations.append({
                "id": aid,
                "image_id": image_id,
                "category_id": 1,
                "bbox": bbox,
                "area": area,
                "iscrowd": 0
            })
            aid += 1
    elapsed = time.time() - start
    print(f"ğŸ•’ {img_path.name} ì¶”ë¡  ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ (person {len(annotations)}ê°œ ê°ì§€)")
    return image_entry, annotations, aid

def run_yolo_and_create_json_parallel(images, output_json_path, model0, model1):
    coco = {
        "images": [],
        "annotations": [],
        "categories": [{"id": 1, "name": "person", "supercategory": "object"}]
    }
    def process(i_img_ann):
        i, (img_path, aid) = i_img_ann
        model = model0 if i % 2 == 0 else model1
        return run_yolo_on_image(model, img_path, i + 1, aid)
    start_all = time.time()
    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(process, enumerate([(img, i * 1000 + 1) for i, img in enumerate(images)]))
    for image_entry, anns, _ in results:
        coco["images"].append(image_entry)
        coco["annotations"].extend(anns)
    with open(output_json_path, "w") as f:
        json.dump(coco, f, indent=2)
    print(f"âœ… ì „ì²´ YOLO ì¶”ë¡  ì‹œê°„: {time.time() - start_all:.2f}ì´ˆ")

# ====== ë©”ì¸ íŒŒì´í”„ë¼ì¸ ======
def compress_and_upload_all(
    image_root_dir: Path,
    project_id,
    headers,
    project_name,
    organization="",
    batch_size=100,
    org_slug="",
    exclude_users: Optional[Set[str]] = None,
):
    """
    [ê¸°ëŠ¥ ìš”ì•½]
      - ì´ë¯¸ì§€ í´ë” íŠ¸ë¦¬ ìˆœíšŒ
      - (bboxes / keypoints í´ë”ê°€ ìˆëŠ” í´ë”ëŠ” ìŠ¤í‚µ)
      - ë°°ì¹˜ ë‹¨ìœ„ë¡œ ZIP ìƒì„± + YOLO(person) ê°ì§€ â†’ COCO JSON ìƒì„±
      - CVAT Task ìƒì„± â†’ ZIP ì—…ë¡œë“œ â†’ (í”„ë ˆì„ ì¸ë±ì‹± ëŒ€ê¸°) â†’ COCO 1.0 ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ
      - ì—…ë¡œë“œ ì§í›„ ì„œë²„ ë©”íƒ€ ë¦¬í”„ë ˆì‹œ/ì¡°íšŒ
      - ğŸ”¹ membershipsì—ì„œ role='worker' ì „ì²´ë¥¼ ë¶ˆëŸ¬ì™€ (ì œì™¸ ëª©ë¡ ì œê±° í›„) **ë¼ìš´ë“œë¡œë¹ˆ by job** ë¶„ë°°
      - ëª¨ë“  ë‹¨ê³„ ì„±ê³µ ì‹œ, ìƒì„±í•œ .json / .zip íŒŒì¼ ì‚­ì œ
    """
    exclude_users = exclude_users or set()

    # --- ë‘ ê°œì˜ YOLO ëª¨ë¸ì„ ì„œë¡œ ë‹¤ë¥¸ GPUì— ì˜¬ë ¤ ë³‘ë ¬ ì¶”ë¡  ---
    model0 = YOLO("yolov8s.pt").to("cuda:0")
    model1 = YOLO("yolov8s.pt").to("cuda:1")

    # --- 1) role=worker ì§‘í•©(í•œ ë²ˆë§Œ ì¡°íšŒí•˜ì—¬ ìºì‹œ) ---
    worker_usernames = get_worker_usernames(headers, org_slug)
    if not worker_usernames:
        raise ValueError("â›” ì¡°ì§ì— role='worker' ì‚¬ìš©ìê°€ ì—†ìŠµë‹ˆë‹¤.")

    # --- 2) ì œì™¸ ëª©ë¡ ì œê±°í•œ ìµœì¢… í›„ë³´ ë¦¬ìŠ¤íŠ¸(ì›ë³¸ ìˆœì„œì˜ ì˜ë¯¸ê°€ ì—†ìœ¼ë¯€ë¡œ ì •ë ¬ë¡œ ê³ ì •ì„± ë¶€ì—¬) ---
    #     - ì•ˆì •ì  ìˆœì„œë¥¼ ìœ„í•´ ì•ŒíŒŒë²³ ì •ë ¬; ë¼ìš´ë“œë¡œë¹ˆì€ ì´ ìˆœì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìˆœí™˜
    eligible_assignees = sorted([u for u in worker_usernames if u not in (exclude_users or set())])
    if not eligible_assignees:
        raise ValueError(
            f"â›” ì œì™¸ ëª©ë¡ì„ ì ìš©í•˜ë‹ˆ ë°°ë¶„í•  ì›Œì»¤ê°€ ì—†ìŠµë‹ˆë‹¤. exclude={sorted(exclude_users)}"
        )

    print(f"âœ… GPU ì‚¬ìš© í™•ì¸: {torch.cuda.get_device_name(0)}, {torch.cuda.get_device_name(1)}")
    print(f"ğŸ‘· ìµœì¢… ëŒ€ìƒ(ì›Œì»¤ & ì œì™¸ë°˜ì˜): {eligible_assignees}")

    # --- ìƒìœ„ image_root_dir ì´í•˜ ëª¨ë“  í•˜ìœ„ í´ë” ìˆœíšŒ ---
    for group_dir in image_root_dir.rglob("*"):
        if not group_dir.is_dir():
            continue

        # ì•ˆì „ì¥ì¹˜: ë¼ë²¨ ì‚°ì¶œë¬¼ í´ë” ìŠ¤í‚µ
        if any((group_dir / skip_name).exists() for skip_name in ["bboxes", "keypoints"]):
            print(f"â© ìŠ¤í‚µ: {group_dir} (í•˜ìœ„ì— bboxes ë˜ëŠ” keypoints í´ë” ì¡´ì¬)")
            continue

        # ì´ë¯¸ì§€ íŒŒì¼ë§Œ ìˆ˜ì§‘
        image_files = sorted([
            f for f in group_dir.glob("*")
            if f.suffix.lower() in [".jpg", ".jpeg", ".png", ".bmp"]
        ])
        if not image_files:
            continue

        # ë°°ì¹˜ ë‚˜ëˆ„ê¸°
        num_batches = ceil(len(image_files) / batch_size)

        for i in range(num_batches):
            batch_files = image_files[i * batch_size : (i + 1) * batch_size]

            zip_filename = f"{group_dir.name}_{i+1:02d}.zip"
            json_filename = f"{group_dir.name}_{i+1:02d}.json"
            zip_path = group_dir / zip_filename
            json_path = group_dir / json_filename

            # 1) YOLO ê°ì§€ + COCO JSON ìƒì„±
            run_yolo_and_create_json_parallel(batch_files, json_path, model0, model1)

            # 2) ZIP ì••ì¶•
            with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zipf:
                for img_path in batch_files:
                    zipf.write(img_path, arcname=img_path.name)
            print(f"[Batch] {zip_filename} created with {len(batch_files)} images")

            # 3) Task ìƒì„± + ZIP ì—…ë¡œë“œ
            task_name = zip_path.stem
            try:
                task_id = create_task_with_zip(task_name, project_id, zip_path, headers, org_slug=org_slug)
            except Exception as e:
                print(f"âŒ Task ìƒì„±/ZIP ì—…ë¡œë“œ ì‹¤íŒ¨: {task_name} | ì—ëŸ¬: {e}")
                continue

            # 4) í”„ë ˆì„ ì¸ë±ì‹± ëŒ€ê¸°
            if not wait_until_task_ready(task_id, headers, org_slug):
                print(f"[CVAT] Task {task_name} ì´ˆê¸°í™” ì‹¤íŒ¨(í”„ë ˆì„ ì¸ë±ì‹± ë¯¸ì™„ë£Œ)")
                continue

            # 5) COCO 1.0 ì–´ë…¸ ì—…ë¡œë“œ
            ok = upload_annotations(task_id, json_path, headers, org_slug)
            if not ok:
                print(f"[CVAT] Task {task_name} ì–´ë…¸ ì—…ë¡œë“œ ì‹¤íŒ¨")
                continue

            # 6) ì„œë²„ ë©”íƒ€ ë¦¬í”„ë ˆì‹œ/ì¡°íšŒ
            refresh_and_check_counts(task_id, headers, org_slug)
            print(f"[CVAT] Task {task_name} ë“±ë¡ ë° ì–´ë…¸í…Œì´ì…˜ ì™„ë£Œ")

            # 7) ğŸ”¹ ì‘ì—…ì ë¼ìš´ë“œë¡œë¹ˆ by job ë¶„ë°° (ëª¨ë“  ì›Œì»¤ì—ê²Œ ê· ë“± ë¶„ë°°)
            try:
                jobs = get_jobs(task_id, headers, org_slug)
                counts = assign_jobs_round_robin(
                    jobs=jobs,
                    headers=headers,
                    assignees=eligible_assignees,
                    org_slug=org_slug,
                )
                # ì‚¬ìš©ìë³„ ë°°ë¶„ ê²°ê³¼ë¥¼ ë¡œê·¸ì— ê¸°ë¡ (ì—¬ëŸ¬ ì¤„)
                for name, c in counts.items():
                    if c > 0:
                        log_assignment(
                            task_name, task_id, name, c,
                            project_name, organization
                        )
            except Exception as e:
                print(f"âš ï¸ ì‘ì—…ì ë¶„ë°°(ë¼ìš´ë“œë¡œë¹ˆ) ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

            # 8) ì‚°ì¶œë¬¼(.json / .zip) ì‚­ì œ
            try:
                if json_path.exists():
                    os.remove(json_path)
                    print(f"ğŸ—‘ï¸ Deleted JSON: {json_path}")
                if zip_path.exists():
                    os.remove(zip_path)
                    print(f"ğŸ—‘ï¸ Deleted ZIP: {zip_path}")
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ====== Entry Point ======
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="YOLO ë³‘ë ¬ ì¶”ë¡  + COCO JSON + CVAT ìë™ ì—…ë¡œë“œ")
    parser.add_argument("--org_name", required=True)
    parser.add_argument("--image_dir", required=True)
    parser.add_argument("--project_name", required=True)
    parser.add_argument("--labels", type=str, nargs="+", required=True)
    # --assignees ì œê±°!
    parser.add_argument("--exclude_users", type=str, nargs="*", default=[], help="í• ë‹¹ì—ì„œ ì œì™¸í•  username ëª©ë¡")
    args = parser.parse_args()
    
    ORGANIZATION = args.org_name

    # .env ì˜ ORGANIZATIONS ì²´í¬
    if args.org_name not in ORGANIZATIONS:
        raise ValueError(f"âŒ ì§€ì •ëœ ì¡°ì§({args.org_name})ì´ .envì˜ ì¡°ì§ ë¦¬ìŠ¤íŠ¸ì— ì—†ìŠµë‹ˆë‹¤: {ORGANIZATIONS}")
    
    image_dir = Path(args.image_dir)
    org_id, org_slug = get_or_create_organization(args.org_name)
    headers = build_headers(org_slug)

    # ì¸ì¦/ì¡°ì§ ì»¨í…ìŠ¤íŠ¸ ì‚¬ì „ í™•ì¸
    preflight_check(headers, org_slug)

    # í”„ë¡œì íŠ¸ ìƒì„±(ì¤‘ë³µ ë¼ë²¨ ë°©ì§€)
    project_id = create_project(args.project_name, labels=args.labels, headers=headers, org_slug=org_slug)

    # ë³¸ ì²˜ë¦¬
    exclude_set = set(args.exclude_users or [])
    compress_and_upload_all(
        image_dir, project_id, headers,
        project_name=args.project_name,
        organization=ORGANIZATION, batch_size=100, org_slug=org_slug,
        exclude_users=exclude_set
    )
