import os, json, zipfile, argparse, torch, time
from pathlib import Path
from ultralytics import YOLO
from PIL import Image
from dotenv import load_dotenv
from datetime import datetime
import requests, colorsys
from math import ceil
from concurrent.futures import ThreadPoolExecutor
import pandas as pd
import csv
from itertools import cycle


env_path = Path(__file__).resolve().parent.parent / ".env"
load_dotenv(dotenv_path=env_path)
CVAT_URL = os.getenv("CVAT_URL_2")
TOKEN = os.getenv("TOKEN_2")
ORGANIZATIONS = [org.strip() for org in os.getenv("ORGANIZATIONS", "").split(",")]
ASSIGN_LOG_PATH = Path(f"./logs/assignments_log.csv")
ASSIGN_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)


def hsv_to_hex(h, s, v):
    r, g, b = colorsys.hsv_to_rgb(h, s, v)
    return '#{:02X}{:02X}{:02X}'.format(int(r * 255), int(g * 255), int(b * 255))

# === CVAT API ===
def get_or_create_organization(name):
    headers = {"Authorization": f"Token {TOKEN}", "Content-Type": "application/json"}
    res = requests.get(f"{CVAT_URL}/api/organizations", headers=headers)
    res.raise_for_status()
    for org in res.json()["results"]:
        if org["slug"] == name or org["name"] == name:
            return org["id"], org["slug"]
    slug = name.lower().replace(" ", "-")
    res = requests.post(f"{CVAT_URL}/api/organizations", headers=headers, json={"name": name, "slug": slug})
    res.raise_for_status()
    return res.json()["id"], slug

def build_headers(org_slug):
    return {
        "Authorization": f"Token {TOKEN}",
        "Content-Type": "application/json",
        "X-Organization": org_slug
    }

def create_project(name, labels, headers):
    label_defs = [{"name": label, "color": hsv_to_hex(i/len(labels), 0.7, 0.95)} for i, label in enumerate(labels)]
    res = requests.post(f"{CVAT_URL}/api/projects", headers=headers, json={"name": name, "labels": label_defs})
    res.raise_for_status()
    return res.json()["id"]

def create_task_with_zip(name, project_id, zip_path, headers):
    res = requests.post(f"{CVAT_URL}/api/tasks", headers=headers, json={
        "name": name, "project_id": project_id, "image_quality": 100, "segment_size": 100
    })
    res.raise_for_status()
    task_id = res.json()["id"]

    upload_headers = headers.copy()
    upload_headers.pop("Content-Type", None)
    with open(zip_path, "rb") as zip_file:
        files = {"client_files[0]": (os.path.basename(zip_path), zip_file, "application/zip")}
        data = {
            "image_quality": 100,
            "use_zip_chunks": "false",
            "use_cache": "false",
            "sorting_method": "lexicographical",
            "upload_format": "zip"
        }
        res = requests.post(f"{CVAT_URL}/api/tasks/{task_id}/data", headers=upload_headers, files=files, data=data)
        res.raise_for_status()
    return task_id

def wait_until_task_ready(task_id, headers, timeout=120):
    start = time.time()
    while time.time() - start < timeout:
        res = requests.get(f"{CVAT_URL}/api/tasks/{task_id}", headers=headers)
        if res.status_code != 200:
            print(f"âŒ Task ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {res.status_code}")
            break
        task_info = res.json()
        if task_info.get("size", 0) > 0:
            return True
        time.sleep(2)
    return False

def upload_annotations(task_id, json_path, headers):
    print(f"â³ ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ ì‹œì‘: Task ID {task_id}")
    upload_headers = headers.copy()
    upload_headers.pop("Content-Type", None)
    with open(json_path, "rb") as jf:
        files = {"annotation_file": (json_path.name, jf, "application/json")}
        url = f"{CVAT_URL}/api/tasks/{task_id}/annotations"
        params = {
            "org": ORGANIZATIONS,
            "format": "COCO 1.0",
            "filename": json_path.name,
            "conv_mask_to_poly": "true"
        }
        # print(f"[DEBUG] ìš”ì²­ URL: {url}")
        # print(f"[DEBUG] ìš”ì²­ Params: {params}")
        # print(f"[DEBUG] ì—…ë¡œë“œ íŒŒì¼ëª…: {json_path.name}")

        res = requests.put(url, headers=upload_headers, files=files, params=params)
        # print(f"[DEBUG] ìš”ì²­ URL (res.url): {res.url}")

    if res.status_code in [200, 202]:
        print(f"âœ… ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ ì„±ê³µ: Task {task_id}")
    else:
        print(f"âŒ ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {res.status_code}")
        print(res.text)


def get_jobs(task_id, headers):
    res = requests.get(f"{CVAT_URL}/api/jobs?task_id={task_id}", headers=headers)
    res.raise_for_status()
    return res.json()["results"]

def get_user_id(username, headers):
    res = requests.get(f"{CVAT_URL}/api/users", headers=headers)
    res.raise_for_status()
    for user in res.json()["results"]:
        if user["username"] == username:
            return user["id"]
    return None

def assign_jobs_to_one_user(jobs, headers, assignee_name):
    user_id = get_user_id(assignee_name, headers)
    if not user_id:
        print(f"âŒ ì‚¬ìš©ì '{assignee_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    for job in jobs:
        if job.get("assignee"): continue
        try:
            res = requests.patch(f"{CVAT_URL}/api/jobs/{job['id']}", headers=headers, json={"assignee": user_id})
            res.raise_for_status()
            print(f"âœ… Job {job['id']} â†’ '{assignee_name}' í• ë‹¹ ì™„ë£Œ")
        except requests.HTTPError as e:
            print(f"âš ï¸ í• ë‹¹ ì‹¤íŒ¨: {e.response.status_code} - {e.response.text}")

def get_user_display_name(username):
    return os.getenv(f"USERMAP_{username}", username)

## ê¸°ì¡´ ì½”ë“œ
# def log_assignment(task_name, task_id, assignee_name, num_jobs):

#     now = datetime.now().strftime("%d/%m/%Y %H:%M")
#     display_name = get_user_display_name(assignee_name)
#     log_entry = [now, task_name, task_id, display_name, num_jobs]
#     log_columns = ["timestamp", "task_name", "task_id", "assignee", "num_jobs"]

#     if not ASSIGN_LOG_PATH.exists():
#         with open(ASSIGN_LOG_PATH, mode="w", newline="", encoding="utf-8") as f:
#             writer = csv.writer(f)
#             writer.writerow(log_columns)
#             writer.writerow(log_entry)
#         return

#     # ê¸°ì¡´ ë¡œê·¸ ì½ê¸°
#     with open(ASSIGN_LOG_PATH, mode="r", newline="", encoding="utf-8") as f:
#         reader = list(csv.reader(f))
#         header = reader[0]
#         rows = reader[1:]
    
#     # timestamp ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœì •ë ¬
#     rows.sort(key=lambda r: datetime.strptime(r[0], "%d/%m/%Y %H:%M"), reverse=True)

#     # ë‹¤ì‹œ ì €ì¥
#     with open(ASSIGN_LOG_PATH, mode="w", newline="", encoding="utf-8") as f:
#         writer = csv.writer(f)
#         writer.writerow(header)
#         writer.writerows(rows)

def log_assignment(task_name, task_id, assignee_name, num_jobs, project_name):
    
    now_dt = datetime.now()
    now_str = now_dt.strftime("%Y/%m/%d %H:%M")

    log_columns = [
        "timestamp", "organization", "project",
        "task_name", "task_id", "assignee", "num_jobs"
    ]
    display_name = get_user_display_name(assignee_name)

    log_entry_dict = {
        "timestamp": now_str,
        "organization": ORGANIZATIONS,
        "project": project_name,
        "task_name": task_name,
        "task_id": task_id,
        "assignee": display_name,
        "num_jobs": num_jobs
    }

    if not ASSIGN_LOG_PATH.exists():
        with open(ASSIGN_LOG_PATH, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=log_columns)
            writer.writeheader()
            writer.writerow(log_entry_dict)
        return
    
    # ê¸°ì¡´ íŒŒì¼ ì½ê¸°
    with open(ASSIGN_LOG_PATH, "r", newline="", encoding="utf-8") as f:
        reader = list(csv.DictReader(f))
        rows = []

        for r in reader:
            completed_row = {col: r.get(col, "") for col in log_columns}
            rows.append(completed_row)

    # ìƒˆ í•­ëª© ì¶”ê°€
    rows.append(log_entry_dict)

    # timestamp íŒŒì‹± í•¨ìˆ˜
    def parse_timestamp(ts):
        for fmt in ("%d/%m/%Y %H:%M", "%Y/%m/%d %H:%M"):
            try:
                return datetime.strptime(ts, fmt)
            except ValueError:
                continue
        print(f"[âš ï¸] ì˜ëª»ëœ ë‚ ì§œ í¬ë§·: {ts}")
        return datetime.min
    
    # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    rows.sort(key=lambda r: parse_timestamp(r["timestamp"]), reverse=True)

    # ë‹¤ì‹œ ì €ì¥
    with open(ASSIGN_LOG_PATH, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=log_columns)
        writer.writeheader()
        writer.writerows(rows)

# # ë³€ê²½ì´ ì™„ë£Œë˜ë©´ ì´ê±¸ë¡œ ë³€ê²½í•´ì•¼í•¨
# def log_assignment(task_name, task_id, assignee_name, num_jobs, project_name):
#     now_str = datetime.now().strftime("%Y/%m/%d %H:%M")
#     display_name = get_user_display_name(assignee_name)

#     log_columns = [
#         "timestamp", "organization", "project",
#         "task_name", "task_id", "assignee", "num_jobs"
#     ]

#     new_entry = {
#         "timestamp": now_str,
#         "organization": ORGANIZATION,
#         "project" : project_name,
#         "task_name": task_name,
#         "task_id": task_id,
#         "assignee": display_name,
#         "num_jobs": num_jobs
#     }

#     rows = []

#     # ê¸°ì¡´ íŒŒì¼ì´ ìˆë‹¤ë©´ ì½ê³  append
#     if ASSIGN_LOG_PATH.exists():
#         with open(ASSIGN_LOG_PATH, "r", newline="", encoding="utf-8") as f:
#             reader = csv.DictReader(f)
#             rows = list(reader)
    
#     rows.append(new_entry)

#     # timestamp ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
#     rows.sort(key=lambda r: datetime.strptime(r["timestamp"], "%Y/%m/%d %H:%M"), reverse=True)

#     # ì •ë ¬ëœ ë¡œê·¸ ë‹¤ì‹œ ì „ì²´ ì €ì¥
#     with open(ASSIGN_LOG_PATH, "w", newline="", encoding="utf-8") as f:
#         writer = csv.DictWriter(f, fieldnames=log_columns)
#         writer.writeheader()
#         writer.writerows(rows)


def run_yolo_on_image(model, img_path, image_id, annotation_id_start):
    start = time.time()
    img = Image.open(img_path)
    width, height = img.size
    image_entry = {
        "id": image_id,
        "file_name": img_path.name,
        "width": width,
        "height": height
    }
    results = model(img_path)
    annotations = []
    aid = annotation_id_start
    for r in results:
        for i, box in enumerate(r.boxes):
            cls_id = int(r.boxes.cls[i].item())
            cls_name = r.names[cls_id]
            if cls_name != "person":
                continue
            x1, y1, x2, y2 = map(float, box.xyxy[0])
            bbox = [x1, y1, x2 - x1, y2 - y1]
            area = bbox[2] * bbox[3]
            annotations.append({
                "id": aid,
                "image_id": image_id,
                "category_id": 1,
                "bbox": bbox,
                "area": area,
                "iscrowd": 0
            })
            aid += 1
    elapsed = time.time() - start
    print(f"ğŸ•’ {img_path.name} ì¶”ë¡  ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ (person {len(annotations)}ê°œ ê°ì§€)")
    return image_entry, annotations, aid

def run_yolo_and_create_json_parallel(images, output_json_path, model0, model1):
    coco = {
        "images": [],
        "annotations": [],
        "categories": [{"id": 1, "name": "person", "supercategory": "object"}]
    }
    def process(i_img_ann):
        i, (img_path, aid) = i_img_ann
        model = model0 if i % 2 == 0 else model1
        return run_yolo_on_image(model, img_path, i + 1, aid)
    start_all = time.time()
    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(process, enumerate([(img, i * 1000 + 1) for i, img in enumerate(images)]))
    for image_entry, anns, _ in results:
        coco["images"].append(image_entry)
        coco["annotations"].extend(anns)
    with open(output_json_path, "w") as f:
        json.dump(coco, f, indent=2)
    print(f"âœ… ì „ì²´ YOLO ì¶”ë¡  ì‹œê°„: {time.time() - start_all:.2f}ì´ˆ")

def compress_and_upload_all(image_root_dir: Path, project_id, headers, assignees, project_name, batch_size=100):
    model0 = YOLO("yolov8s.pt").to("cuda:0")
    model1 = YOLO("yolov8s.pt").to("cuda:1")
    # num_users = len(assignees)
    assignee_cycle = cycle(assignees)
    print(f"âœ… GPU ì‚¬ìš© í™•ì¸: {torch.cuda.get_device_name(0)}, {torch.cuda.get_device_name(1)}")
    for group_dir in image_root_dir.rglob("*"):
        if not group_dir.is_dir(): continue
        image_files = sorted([f for f in group_dir.glob("*") if f.suffix.lower() in [".jpg", ".jpeg", ".png", ".bmp"]])
        if not image_files: continue
        num_batches = ceil(len(image_files) / batch_size)
        for i in range(num_batches):
            batch_files = image_files[i * batch_size : (i + 1) * batch_size]
            zip_filename = f"{group_dir.name}_{i+1:02d}.zip"
            json_filename = f"{group_dir.name}_{i+1:02d}.json"
            zip_path = group_dir / zip_filename
            json_path = group_dir / json_filename
            run_yolo_and_create_json_parallel(batch_files, json_path, model0, model1)
            with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zipf:
                for img_path in batch_files:
                    zipf.write(img_path, arcname=img_path.name)
            print(f"[Batch] {zip_filename} created with {len(batch_files)} images")
            task_name = zip_path.stem
            task_id = create_task_with_zip(task_name, project_id, zip_path, headers)
            if wait_until_task_ready(task_id, headers):
                upload_annotations(task_id, json_path, headers)
                print(f"[CVAT] Task {task_name} ë“±ë¡ ë° ì–´ë…¸í…Œì´ì…˜ ì™„ë£Œ")
            else:
                print(f"[CVAT] Task {task_name} ì´ˆê¸°í™” ì‹¤íŒ¨")
                continue
            jobs = get_jobs(task_id, headers)
            # assignee_name = assignees[i % num_users] if num_users > 0 else None
            assignee_name = next(assignee_cycle)
            if assignee_name:
                assign_jobs_to_one_user(jobs, headers, assignee_name)
                log_assignment(task_name, task_id, assignee_name, len(jobs), project_name)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="YOLO ë³‘ë ¬ ì¶”ë¡  + COCO JSON + CVAT ìë™ ì—…ë¡œë“œ")
    parser.add_argument("--org_name", required=True)
    parser.add_argument("--image_dir", required=True)
    parser.add_argument("--project_name", required=True)
    parser.add_argument("--labels", type=str, nargs="+", required=True)
    parser.add_argument("--assignees", type=str, nargs="+", required=True)
    args = parser.parse_args()

    if args.org_name not in ORGANIZATIONS:
        raise ValueError(f"âŒ ì§€ì •ëœ ì¡°ì§({args.org_name})ì´ .envì˜ ì¡°ì§ ë¦¬ìŠ¤íŠ¸ì— ì—†ìŠµë‹ˆë‹¤: {ORGANIZATIONS}")
    
    image_dir = Path(args.image_dir)
    org_id, org_slug = get_or_create_organization(args.org_name)
    headers = build_headers(org_slug)
    project_id = create_project(args.project_name, labels=args.labels, headers=headers)
    compress_and_upload_all(image_dir, project_id, headers, assignees=args.assignees, project_name=args.project_name, batch_size=100)
