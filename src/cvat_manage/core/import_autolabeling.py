import os, json, zipfile, argparse, torch, time
from pathlib import Path
from ultralytics import YOLO
from PIL import Image
from dotenv import load_dotenv
from datetime import datetime
import requests, colorsys
from math import ceil
from concurrent.futures import ThreadPoolExecutor
import pandas as pd
import csv
from itertools import cycle

# ====== ENV ======
env_path = Path(__file__).resolve().parent.parent / ".env"
load_dotenv(dotenv_path=env_path)
CVAT_URL = os.getenv("CVAT_URL_2")
TOKEN = os.getenv("TOKEN_2")
ORGANIZATIONS = [org.strip() for org in os.getenv("ORGANIZATIONS", "").split(",")]
ASSIGN_LOG_PATH = Path(f"./logs/assignments_log.csv")
ASSIGN_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)

# ====== Utils ======
def hsv_to_hex(h, s, v):
    r, g, b = colorsys.hsv_to_rgb(h, s, v)
    return '#{:02X}{:02X}{:02X}'.format(int(r * 255), int(g * 255), int(b * 255))

def _debug_http_error(prefix, res):
    print(f"[{prefix}] status={res.status_code}")
    try:
        print(f"[{prefix}] body.json=", res.json())
    except Exception:
        print(f"[{prefix}] body.text=", res.text)

def _safe_json(res):
    try:
        return res.json()
    except Exception:
        return {"_text": res.text}

# ====== CVAT API ======
def get_or_create_organization(name):
    """ì¡°ì§ ì¡°íšŒ/ìƒì„±. (í—¤ë”ì— org ë¯¸í¬í•¨)"""
    headers = {"Authorization": f"Token {TOKEN}", "Content-Type": "application/json"}
    res = requests.get(f"{CVAT_URL}/api/organizations", headers=headers)
    res.raise_for_status()
    for org in res.json().get("results", []):
        if org["slug"] == name or org["name"] == name:
            return org["id"], org["slug"]
    slug = name.lower().replace(" ", "-")
    res = requests.post(f"{CVAT_URL}/api/organizations", headers=headers, json={"name": name, "slug": slug})
    res.raise_for_status()
    return res.json()["id"], slug

def build_headers(org_slug):
    """ê³µí†µ í—¤ë”: ì¼ë¶€ ë°°í¬ì—ì„œ ì»¤ìŠ¤í…€ í—¤ë” ë“œë¡­ ë°©ì§€ë¥¼ ìœ„í•´ ì¿¼ë¦¬ìŠ¤íŠ¸ë§ë„ ë³‘í–‰ ì‚¬ìš©"""
    return {
        "Authorization": f"Token {TOKEN}",
        "Content-Type": "application/json",
        "X-Organization": org_slug
    }

def preflight_check(headers, org_slug):
    """ë™ì¼ ì»¨í…ìŠ¤íŠ¸ë¡œ /api/tasks ì ‘ê·¼ì´ í—ˆìš©ë˜ëŠ”ì§€ ì‚¬ì „ í™•ì¸"""
    url = f"{CVAT_URL}/api/tasks?org={org_slug}"
    try:
        res = requests.get(url, headers=headers)
        if res.status_code == 200:
            print("âœ… Preflight OK: /api/tasks GET authorized with org context")
            return True
        else:
            _debug_http_error("Preflight /api/tasks", res)
            return False
    except Exception as e:
        print("âŒ Preflight exception:", e)
        return False

def _normalize_and_dedupe_labels(labels):
    """ë¼ë²¨ ì •ê·œí™”+ì¤‘ë³µ ì œê±°(ìˆœì„œ ë³´ì¡´) ë° ê¸°ë³¸ attributes í¬í•¨"""
    seen = set()
    uniq = []
    for raw in labels:
        lbl = str(raw).strip()
        if not lbl:
            continue
        if lbl in seen:
            print(f"[WARN] Duplicate label skipped: '{lbl}'")
            continue
        seen.add(lbl)
        uniq.append(lbl)
    if not uniq:
        raise ValueError("No valid labels after normalization. Check --labels arguments.")
    label_defs = []
    for i, lbl in enumerate(uniq):
        label_defs.append({
            "name": lbl,
            "color": hsv_to_hex(i / max(1, len(uniq)), 0.7, 0.95),
            "attributes": []  # ì¼ë¶€ ë²„ì „ì—ì„œ í•„ìˆ˜
        })
    return label_defs, uniq

def create_project(name, labels, headers, org_slug):
    """í”„ë¡œì íŠ¸ ìƒì„±(ì¤‘ë³µ ë¼ë²¨ ë°©ì§€, org ì¿¼ë¦¬ ë³‘í–‰)"""
    label_defs, uniq_labels = _normalize_and_dedupe_labels(labels)
    base = f"{CVAT_URL}/api/projects"
    url_candidates = [f"{base}/?org={org_slug}", f"{base}?org={org_slug}", f"{base}/", base]
    last_err = None
    for url in url_candidates:
        res = requests.post(url, headers=headers, json={"name": name, "labels": label_defs})
        if res.status_code >= 400:
            _debug_http_error(f"Project create POST {url}", res)
        try:
            res.raise_for_status()
            pid = res.json()["id"]
            print(f"âœ… Project created via {url} â†’ id={pid}, labels={uniq_labels}")
            return pid
        except requests.HTTPError as e:
            last_err = e
    raise last_err

def create_task_with_zip(name, project_id, zip_path, headers, org_slug):
    """Task ìƒì„± + ZIP ì—…ë¡œë“œ. org ì¿¼ë¦¬ ë³‘í–‰ ë° íŠ¸ë ˆì¼ë§ ìŠ¬ë˜ì‹œ í˜¸í™˜."""
    base = f"{CVAT_URL}/api/tasks"
    url_candidates = [f"{base}/?org={org_slug}", f"{base}?org={org_slug}", f"{base}/", base]
    payload = {
        "name": name,
        "project_id": project_id,
        "image_quality": 100,
        "segment_size": 100
    }
    task_id = None
    last_err = None
    for url in url_candidates:
        res = requests.post(url, headers=headers, json=payload)
        if res.status_code >= 400:
            _debug_http_error(f"Task create POST {url}", res)
        try:
            res.raise_for_status()
            task_id = res.json()["id"]
            print(f"âœ… Task created via {url} â†’ id={task_id}")
            break
        except requests.HTTPError as e:
            last_err = e
            continue
    if task_id is None:
        raise last_err

    # ---- ë°ì´í„° ì—…ë¡œë“œ (files ì—…ë¡œë“œ ì‹œ Content-Type ì œê±° í•„ìˆ˜) ----
    upload_headers = headers.copy()
    upload_headers.pop("Content-Type", None)
    with open(zip_path, "rb") as zip_file:
        files = {"client_files[0]": (os.path.basename(zip_path), zip_file, "application/zip")}
        data = {
            "image_quality": 100,
            "use_zip_chunks": "false",
            "use_cache": "false",
            "sorting_method": "lexicographical",
            "upload_format": "zip"
        }
        data_url = f"{CVAT_URL}/api/tasks/{task_id}/data?org={org_slug}"
        res = requests.post(data_url, headers=upload_headers, files=files, data=data)
        if res.status_code >= 400:
            _debug_http_error("Task data upload", res)
        res.raise_for_status()
        print(f"ğŸ“¦ Uploaded ZIP to task {task_id}")

    return task_id

def wait_until_task_ready(task_id, headers, org_slug, timeout=120):
    """í”„ë ˆì„ ì¸ë±ì‹± ì™„ë£Œê¹Œì§€ ëŒ€ê¸° (size>0)"""
    start = time.time()
    while time.time() - start < timeout:
        res = requests.get(f"{CVAT_URL}/api/tasks/{task_id}?org={org_slug}", headers=headers)
        if res.status_code != 200:
            print(f"âŒ Task ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {res.status_code}")
            break
        task_info = res.json()
        if task_info.get("size", 0) > 0:
            return True
        time.sleep(2)
    return False

def upload_annotations(task_id, json_path, headers, org_slug):
    """COCO 1.0 ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ (ì •í™•í•œ org ì „ë‹¬)"""
    print(f"â³ ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ ì‹œì‘: Task ID {task_id}")
    upload_headers = headers.copy()
    upload_headers.pop("Content-Type", None)
    with open(json_path, "rb") as jf:
        files = {"annotation_file": (json_path.name, jf, "application/json")}
        url = f"{CVAT_URL}/api/tasks/{task_id}/annotations"
        params = {
            "org": org_slug,          # â† ë‹¨ì¼ slug ë¡œ ìˆ˜ì • (ê¸°ì¡´: ë¦¬ìŠ¤íŠ¸ì˜€ìŒ)
            "format": "COCO 1.0",
            "filename": json_path.name,
            "conv_mask_to_poly": "true"
        }
        res = requests.put(url, headers=upload_headers, files=files, params=params)

    if res.status_code in [200, 202]:
        print(f"âœ… ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ ì„±ê³µ: Task {task_id}")
    else:
        print(f"âŒ ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {res.status_code}")
        print(res.text)
        return False
    return True

def refresh_and_check_counts(task_id, headers, org_slug):
    """
    ì‘ì—…ìë“¤ì´ 'Jobì—ì„  ë³´ì´ëŠ”ë° Task/Projectsì—ì„  ì•ˆ ë³´ì„' í˜¼ì„ ì„ ì¤„ì´ê¸° ìœ„í•´
    ì—…ë¡œë“œ ì§í›„ ì„œë²„ ì¸¡ ìš”ì•½ê°’ì„ ê°±ì‹ /ì¡°íšŒ.
    - ì¼ë¶€ ë°°í¬ì—ì„  ì–´ë…¸ ì—…ë¡œë“œ ì§í›„ ì¹´ìš´íŠ¸ê°€ ëŠ¦ê²Œ ë°˜ì˜ë  ìˆ˜ ìˆì–´ ìš”ì²­ìœ¼ë¡œ ë¦¬í”„ë ˆì‹œ ì‹œë„
    """
    # (A) ê°€ëŠ¥í•œ ê²½ìš°: reload ì•¡ì…˜ ì‹œë„ (ë²„ì „ ì˜ì¡´ì , ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ)
    try:
        url_reload = f"{CVAT_URL}/api/tasks/{task_id}/annotations?action=reload&org={org_slug}"
        r = requests.post(url_reload, headers=headers)
        print("ğŸ”„ annotations reload:", r.status_code)
    except Exception as e:
        print("reload skip:", e)

    # (B) Task ë©”íƒ€ ì¬ì¡°íšŒ
    time.sleep(1.0)
    meta = requests.get(f"{CVAT_URL}/api/tasks/{task_id}?org={org_slug}", headers=headers)
    if meta.status_code == 200:
        j = meta.json()
        print(f"ğŸ§¾ Task meta: size={j.get('size')} | segments={j.get('segments')}")
    else:
        _debug_http_error("Task meta refresh", meta)

def get_jobs(task_id, headers, org_slug):
    res = requests.get(f"{CVAT_URL}/api/jobs?task_id={task_id}&org={org_slug}", headers=headers)
    res.raise_for_status()
    return res.json().get("results", [])

# def get_user_id(username, headers, org_slug):
#     res = requests.get(f"{CVAT_URL}/api/users?org={org_slug}", headers=headers)
#     res.raise_for_status()
#     for user in res.json().get("results", []):
#         if user["username"] == username:
#             return user["id"]
#     return None

def get_user_id(username: str, headers: dict, org_slug: str, page_size: int = 100):
    """
    íŠ¹ì • username ì— í•´ë‹¹í•˜ëŠ” user.id ë°˜í™˜
    - ì¡°ì§ ë‚´ ëª¨ë“  í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ê²€ìƒ‰
    - ì—†ìœ¼ë©´ None ë°˜í™˜
    """
    url = f"{CVAT_URL}/api/users?org={org_slug}&page_size={page_size}"

    while url:
        res = requests.get(url, headers=headers)
        res.raise_for_status()
        data = res.json()

        # í˜„ì¬ í˜ì´ì§€ì—ì„œ username ë§¤ì¹­ ê²€ì‚¬
        for user in data.get("results", []) or []:
            if user.get("username") == username:
                return user.get("id")

        # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
        url = data.get("next")

    # ëê¹Œì§€ ëª» ì°¾ìœ¼ë©´ None
    return None


def assign_jobs_to_one_user(jobs, headers, assignee_name, org_slug):
    user_id = get_user_id(assignee_name, headers, org_slug)
    if not user_id:
        print(f"âŒ ì‚¬ìš©ì '{assignee_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    for job in jobs:
        if job.get("assignee"): 
            continue
        try:
            res = requests.patch(
                f"{CVAT_URL}/api/jobs/{job['id']}?org={org_slug}",
                headers=headers,
                json={"assignee": user_id}
            )
            res.raise_for_status()
            print(f"âœ… Job {job['id']} â†’ '{assignee_name}' í• ë‹¹ ì™„ë£Œ")
        except requests.HTTPError as e:
            print(f"âš ï¸ í• ë‹¹ ì‹¤íŒ¨: {e.response.status_code} - {e.response.text}")

def get_user_display_name(username):
    return os.getenv(f"USERMAP_{username}", username)

def log_assignment(task_name, task_id, assignee_name, num_jobs, project_name, organization):
    now_dt = datetime.now()
    now_str = now_dt.strftime("%Y/%m/%d %H:%M")
    log_columns = ["timestamp","organization","project","task_name","task_id","assignee","num_jobs"]
    display_name = get_user_display_name(assignee_name)

    log_entry_dict = {
        "timestamp": now_str,
        "organization": organization,
        "project": project_name,
        "task_name": task_name,
        "task_id": task_id,
        "assignee": display_name,
        "num_jobs": num_jobs
    }

    if not ASSIGN_LOG_PATH.exists():
        with open(ASSIGN_LOG_PATH, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=log_columns)
            writer.writeheader()
            writer.writerow(log_entry_dict)
        return
    
    with open(ASSIGN_LOG_PATH, "r", newline="", encoding="utf-8") as f:
        reader = list(csv.DictReader(f))
        rows = []
        for r in reader:
            completed_row = {col: r.get(col, "") for col in log_columns}
            rows.append(completed_row)

    def parse_timestamp(ts):
        for fmt in ("%d/%m/%Y %H:%M", "%Y/%m/%d %H:%M"):
            try:
                return datetime.strptime(ts, fmt)
            except ValueError:
                continue
        print(f"[âš ï¸] ì˜ëª»ëœ ë‚ ì§œ í¬ë§·: {ts}")
        return datetime.min
    
    rows.append(log_entry_dict)
    rows.sort(key=lambda r: parse_timestamp(r["timestamp"]), reverse=True)

    with open(ASSIGN_LOG_PATH, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=log_columns)
        writer.writeheader()
        writer.writerows(rows)

# ====== YOLO / COCO ìƒì„± ======
def run_yolo_on_image(model, img_path, image_id, annotation_id_start):
    start = time.time()
    img = Image.open(img_path)
    width, height = img.size
    image_entry = {
        "id": image_id,
        "file_name": img_path.name,
        "width": width,
        "height": height
    }
    results = model(img_path)
    annotations = []
    aid = annotation_id_start
    for r in results:
        for i, box in enumerate(r.boxes):
            cls_id = int(r.boxes.cls[i].item())
            cls_name = r.names[cls_id]
            if cls_name != "person":
                continue
            x1, y1, x2, y2 = map(float, box.xyxy[0])
            bbox = [x1, y1, x2 - x1, y2 - y1]
            area = bbox[2] * bbox[3]
            annotations.append({
                "id": aid,
                "image_id": image_id,
                "category_id": 1,
                "bbox": bbox,
                "area": area,
                "iscrowd": 0
            })
            aid += 1
    elapsed = time.time() - start
    print(f"ğŸ•’ {img_path.name} ì¶”ë¡  ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ (person {len(annotations)}ê°œ ê°ì§€)")
    return image_entry, annotations, aid

def run_yolo_and_create_json_parallel(images, output_json_path, model0, model1):
    coco = {
        "images": [],
        "annotations": [],
        "categories": [{"id": 1, "name": "person", "supercategory": "object"}]
    }
    def process(i_img_ann):
        i, (img_path, aid) = i_img_ann
        model = model0 if i % 2 == 0 else model1
        return run_yolo_on_image(model, img_path, i + 1, aid)
    start_all = time.time()
    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(process, enumerate([(img, i * 1000 + 1) for i, img in enumerate(images)]))
    for image_entry, anns, _ in results:
        coco["images"].append(image_entry)
        coco["annotations"].extend(anns)
    with open(output_json_path, "w") as f:
        json.dump(coco, f, indent=2)
    print(f"âœ… ì „ì²´ YOLO ì¶”ë¡  ì‹œê°„: {time.time() - start_all:.2f}ì´ˆ")

# ====== Main flow ======
# def compress_and_upload_all(image_root_dir: Path, project_id, headers, assignees, project_name, organization="", batch_size=100, org_slug=""):
#     # ëª¨ë¸ ë‘ ê°œë¡œ ë³‘ë ¬ ì¶”ë¡ 
#     model0 = YOLO("yolov8s.pt").to("cuda:0")
#     model1 = YOLO("yolov8s.pt").to("cuda:1")
#     assignee_cycle = cycle(assignees)
#     print(f"âœ… GPU ì‚¬ìš© í™•ì¸: {torch.cuda.get_device_name(0)}, {torch.cuda.get_device_name(1)}")
    
#     for group_dir in image_root_dir.rglob("*"):
#         if not group_dir.is_dir():
#             continue

#         # í•˜ìœ„ì— bboxes / keypoints í´ë”ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ
#         if any((group_dir / skip_name).exists() for skip_name in ["bboxes", "keypoints"]):
#             print(f"â© ìŠ¤í‚µ: {group_dir} (í•˜ìœ„ì— bboxes ë˜ëŠ” keypoints í´ë” ì¡´ì¬)")
#             continue
        
#         image_files = sorted([f for f in group_dir.glob("*") if f.suffix.lower() in [".jpg", ".jpeg", ".png", ".bmp"]])
#         if not image_files:
#             continue
        
#         num_batches = ceil(len(image_files) / batch_size)
#         for i in range(num_batches):
#             batch_files = image_files[i * batch_size : (i + 1) * batch_size]
#             zip_filename = f"{group_dir.name}_{i+1:02d}.zip"
#             json_filename = f"{group_dir.name}_{i+1:02d}.json"
#             zip_path = group_dir / zip_filename
#             json_path = group_dir / json_filename
            
#             run_yolo_and_create_json_parallel(batch_files, json_path, model0, model1)
            
#             with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zipf:
#                 for img_path in batch_files:
#                     zipf.write(img_path, arcname=img_path.name)
#             print(f"[Batch] {zip_filename} created with {len(batch_files)} images")
            
#             task_name = zip_path.stem
#             task_id = create_task_with_zip(task_name, project_id, zip_path, headers, org_slug=org_slug)
#             if wait_until_task_ready(task_id, headers, org_slug):
#                 ok = upload_annotations(task_id, json_path, headers, org_slug)
#                 if not ok:
#                     print(f"[CVAT] Task {task_name} ì–´ë…¸ ì—…ë¡œë“œ ì‹¤íŒ¨")
#                     continue
#                 # ì—…ë¡œë“œ í›„ ìš”ì•½ê°’ ê°±ì‹ /í™•ì¸
#                 refresh_and_check_counts(task_id, headers, org_slug)
#                 print(f"[CVAT] Task {task_name} ë“±ë¡ ë° ì–´ë…¸í…Œì´ì…˜ ì™„ë£Œ")
#             else:
#                 print(f"[CVAT] Task {task_name} ì´ˆê¸°í™” ì‹¤íŒ¨")
#                 continue
            
#             jobs = get_jobs(task_id, headers, org_slug)
#             assignee_name = next(assignee_cycle)
#             if assignee_name:
#                 assign_jobs_to_one_user(jobs, headers, assignee_name, org_slug)
#                 log_assignment(task_name, task_id, assignee_name, len(jobs), project_name, organization)

def compress_and_upload_all(
    image_root_dir: Path,
    project_id,
    headers,
    assignees,
    project_name,
    organization="",
    batch_size=100,
    org_slug=""
):
    """
    [ê¸°ëŠ¥ ìš”ì•½]
      - ì´ë¯¸ì§€ í´ë” íŠ¸ë¦¬ ìˆœíšŒ
      - (bboxes / keypoints í´ë”ê°€ ìˆëŠ” í´ë”ëŠ” ìŠ¤í‚µ)  <-- ì•ˆì „ì¥ì¹˜
      - ë°°ì¹˜ ë‹¨ìœ„ë¡œ ZIP ìƒì„± + YOLO(person) ê°ì§€ â†’ COCO JSON ìƒì„±
      - CVAT Task ìƒì„± â†’ ZIP ì—…ë¡œë“œ â†’ (í”„ë ˆì„ ì¸ë±ì‹± ëŒ€ê¸°) â†’ COCO 1.0 ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ
      - ì—…ë¡œë“œ ì§í›„ ì„œë²„ ë©”íƒ€ ë¦¬í”„ë ˆì‹œ/ì¡°íšŒ
      - ì‘ì—…(Job) ëª©ë¡ ì¡°íšŒ í›„ ë¼ìš´ë“œë¡œë¹ˆ ë°©ì‹ìœ¼ë¡œ ì‘ì—…ì í• ë‹¹
      - ğŸ¯ ëª¨ë“  ë‹¨ê³„ ì„±ê³µ ì‹œ, ìƒì„±í•œ .json / .zip íŒŒì¼ ì‚­ì œ

    [ì‚­ì œ ì •ì±…]
      - ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œê¹Œì§€ ì„±ê³µí–ˆì„ ë•Œë§Œ .json/.zip ì‚­ì œ
      - ì‹¤íŒ¨ ì‹œì—ëŠ” ë””ë²„ê¹…ì„ ìœ„í•´ íŒŒì¼ì„ ë‚¨ê¹€
    """
    # --- ë‘ ê°œì˜ YOLO ëª¨ë¸ì„ ì„œë¡œ ë‹¤ë¥¸ GPUì— ì˜¬ë ¤ ë³‘ë ¬ ì¶”ë¡  (ì§/í™€ ì¸ë±ìŠ¤ ë¶„ë°°) ---
    model0 = YOLO("yolov8s.pt").to("cuda:0")
    model1 = YOLO("yolov8s.pt").to("cuda:1")

    # assignees ë¦¬ìŠ¤íŠ¸ë¥¼ ë¼ìš´ë“œë¡œë¹ˆìœ¼ë¡œ ìˆœí™˜
    assignee_cycle = cycle(assignees)

    # GPU ì •ë³´ ì¶œë ¥(ë””ë²„ê¹…ìš©)
    print(f"âœ… GPU ì‚¬ìš© í™•ì¸: {torch.cuda.get_device_name(0)}, {torch.cuda.get_device_name(1)}")

    # --- ìƒìœ„ image_root_dir ì´í•˜ ëª¨ë“  í•˜ìœ„ í´ë” ìˆœíšŒ ---
    for group_dir in image_root_dir.rglob("*"):
        # íŒŒì¼ì€ ê±´ë„ˆë›°ê³ , í´ë”ë§Œ ì²˜ë¦¬
        if not group_dir.is_dir():
            continue

        # â›” ì•ˆì „ì¥ì¹˜: í•˜ìœ„ì— 'bboxes' ë˜ëŠ” 'keypoints' í´ë”ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if any((group_dir / skip_name).exists() for skip_name in ["bboxes", "keypoints"]):
            print(f"â© ìŠ¤í‚µ: {group_dir} (í•˜ìœ„ì— bboxes ë˜ëŠ” keypoints í´ë” ì¡´ì¬)")
            continue

        # ì´ë¯¸ì§€ íŒŒì¼ë§Œ ìˆ˜ì§‘
        image_files = sorted([
            f for f in group_dir.glob("*")
            if f.suffix.lower() in [".jpg", ".jpeg", ".png", ".bmp"]
        ])
        if not image_files:
            continue  # ì´ í´ë”ì— ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ í´ë”ë¡œ

        # --- ë°°ì¹˜ ë‚˜ëˆ„ê¸° ---
        num_batches = ceil(len(image_files) / batch_size)

        for i in range(num_batches):
            # í˜„ì¬ ë°°ì¹˜ì˜ íŒŒì¼ë“¤
            batch_files = image_files[i * batch_size : (i + 1) * batch_size]

            # ë°°ì¹˜ ë‹¨ìœ„ ì‚°ì¶œë¬¼ íŒŒì¼ëª… (ì˜ˆ: í´ë”ëª…_01.zip / í´ë”ëª…_01.json)
            zip_filename = f"{group_dir.name}_{i+1:02d}.zip"
            json_filename = f"{group_dir.name}_{i+1:02d}.json"
            zip_path = group_dir / zip_filename
            json_path = group_dir / json_filename

            # --- 1) YOLO ê°ì§€ + COCO JSON ìƒì„±(ë‘ ëª¨ë¸ì„ ë²ˆê°ˆì•„ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ì²˜ë¦¬) ---
            run_yolo_and_create_json_parallel(batch_files, json_path, model0, model1)

            # --- 2) ZIP ì••ì¶• ìƒì„± ---
            with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zipf:
                for img_path in batch_files:
                    # ZIP ë‚´ë¶€ì—ëŠ” íŒŒì¼ëª…ë§Œ ë„£ì–´ ê²½ë¡œ ë‹¨ìˆœí™”(arcname=íŒŒì¼ëª…)
                    zipf.write(img_path, arcname=img_path.name)
            print(f"[Batch] {zip_filename} created with {len(batch_files)} images")

            # --- 3) CVAT Task ìƒì„± + ZIP ì—…ë¡œë“œ ---
            task_name = zip_path.stem  # í™•ì¥ì ì œì™¸(= ZIP ì´ë¦„)
            try:
                task_id = create_task_with_zip(
                    task_name, project_id, zip_path, headers, org_slug=org_slug
                )
            except Exception as e:
                print(f"âŒ Task ìƒì„±/ZIP ì—…ë¡œë“œ ì‹¤íŒ¨: {task_name} | ì—ëŸ¬: {e}")
                # ì—…ë¡œë“œ ì‹¤íŒ¨ â†’ ë””ë²„ê¹…ì„ ìœ„í•´ íŒŒì¼ ë³´ì¡´
                continue

            # --- 4) Task ì¤€ë¹„(í”„ë ˆì„ ì¸ë±ì‹±) ëŒ€ê¸° ---
            if not wait_until_task_ready(task_id, headers, org_slug):
                print(f"[CVAT] Task {task_name} ì´ˆê¸°í™” ì‹¤íŒ¨(í”„ë ˆì„ ì¸ë±ì‹± ë¯¸ì™„ë£Œ)")
                # ë””ë²„ê¹…ì„ ìœ„í•´ íŒŒì¼ ë³´ì¡´
                continue

            # --- 5) COCO 1.0 ì–´ë…¸í…Œì´ì…˜ ì—…ë¡œë“œ ---
            ok = upload_annotations(task_id, json_path, headers, org_slug)
            if not ok:
                print(f"[CVAT] Task {task_name} ì–´ë…¸ ì—…ë¡œë“œ ì‹¤íŒ¨")
                # ë””ë²„ê¹…ì„ ìœ„í•´ íŒŒì¼ ë³´ì¡´
                continue

            # --- 6) ì„œë²„ ë©”íƒ€ ë¦¬í”„ë ˆì‹œ/ì¡°íšŒ(ìš”ì•½ê°’/ì¹´ìš´íŠ¸ ë°˜ì˜ í™•ì¸ìš©) ---
            refresh_and_check_counts(task_id, headers, org_slug)
            print(f"[CVAT] Task {task_name} ë“±ë¡ ë° ì–´ë…¸í…Œì´ì…˜ ì™„ë£Œ")

            # --- 7) ì‘ì—…ì(Job) í• ë‹¹: ë¼ìš´ë“œë¡œë¹ˆìœ¼ë¡œ í•œ ëª…ì”© ---
            try:
                jobs = get_jobs(task_id, headers, org_slug)
                assignee_name = next(assignee_cycle)
                if assignee_name:
                    assign_jobs_to_one_user(jobs, headers, assignee_name, org_slug)
                    log_assignment(
                        task_name, task_id, assignee_name, len(jobs),
                        project_name, organization
                    )
            except Exception as e:
                # í• ë‹¹ ë‹¨ê³„ì—ì„œ ë¬¸ì œê°€ ìƒê²¨ë„, ê·¸ ì´ì „ ë‹¨ê³„(ì—…ë¡œë“œ/ì–´ë…¸)ëŠ” ì„±ê³µí–ˆìœ¼ë¯€ë¡œ
                # íŒŒì¼ ì‚­ì œëŠ” ì§„í–‰í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤. ë‹¤ë§Œ í•„ìš” ì‹œ ë³´ì¡´í•˜ê³  ì‹¶ë‹¤ë©´ ì—¬ê¸°ì„œ return/continue ë¡œ ë°”ê¿”ë„ ë©ë‹ˆë‹¤.
                print(f"âš ï¸ ì‘ì—…ì í• ë‹¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

            # --- 8) âœ… ëª¨ë“  í•µì‹¬ ë‹¨ê³„ ì„±ê³µ ì‹œ ì‚°ì¶œë¬¼(.json / .zip) ì‚­ì œ ---
            try:
                if json_path.exists():
                    os.remove(json_path)
                    print(f"ğŸ—‘ï¸ Deleted JSON: {json_path}")
                if zip_path.exists():
                    os.remove(zip_path)
                    print(f"ğŸ—‘ï¸ Deleted ZIP: {zip_path}")
            except Exception as e:
                # ì‚­ì œ ìì²´ëŠ” í•„ìˆ˜ ë‹¨ê³„ê°€ ì•„ë‹ˆë¯€ë¡œ ê²½ê³ ë§Œ ë‚¨ê¹€
                print(f"âš ï¸ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="YOLO ë³‘ë ¬ ì¶”ë¡  + COCO JSON + CVAT ìë™ ì—…ë¡œë“œ")
    parser.add_argument("--org_name", required=True)
    parser.add_argument("--image_dir", required=True)
    parser.add_argument("--project_name", required=True)
    parser.add_argument("--labels", type=str, nargs="+", required=True)
    parser.add_argument("--assignees", type=str, nargs="+", required=True)
    args = parser.parse_args()
    
    ORGANIZATION = args.org_name

    # .env ì˜ ORGANIZATIONS ì²´í¬
    if args.org_name not in ORGANIZATIONS:
        raise ValueError(f"âŒ ì§€ì •ëœ ì¡°ì§({args.org_name})ì´ .envì˜ ì¡°ì§ ë¦¬ìŠ¤íŠ¸ì— ì—†ìŠµë‹ˆë‹¤: {ORGANIZATIONS}")
    
    image_dir = Path(args.image_dir)
    org_id, org_slug = get_or_create_organization(args.org_name)
    headers = build_headers(org_slug)

    # ì¸ì¦/ì¡°ì§ ì»¨í…ìŠ¤íŠ¸ ì‚¬ì „ í™•ì¸
    preflight_check(headers, org_slug)

    # í”„ë¡œì íŠ¸ ìƒì„±(ì¤‘ë³µ ë¼ë²¨ ë°©ì§€)
    project_id = create_project(args.project_name, labels=args.labels, headers=headers, org_slug=org_slug)

    # ë³¸ ì²˜ë¦¬
    compress_and_upload_all(
        image_dir, project_id, headers,
        assignees=args.assignees, project_name=args.project_name,
        organization=ORGANIZATION, batch_size=100, org_slug=org_slug
    )
