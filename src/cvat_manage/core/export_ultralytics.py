import os
import csv
import requests
import subprocess
import zipfile
from datetime import datetime
from dotenv import load_dotenv
from pathlib import Path

# === í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ===
env_path = Path(__file__).resolve().parent.parent / ".env"
load_dotenv(dotenv_path=env_path)

CVAT_URL = os.getenv("CVAT_URL")
TOKEN = os.getenv("TOKEN")
CVAT_USERNAME = os.getenv("CVAT_USERNAME")
CVAT_PASSWORD = os.getenv("CVAT_PASSWORD")
CVAT_EXPORT_FORMAT = os.getenv("CVAT_EXPORT_FORMAT_2")
WITH_IMAGES = os.getenv("WITH_IMAGES")
ORG_FILTER = os.getenv("ORGANIZATION_FILTER")
RESULT_DIR = os.getenv("RESULT_DIR", "/tmp/cvat_exports")

HEADERS = {
    "Authorization": f"Token {TOKEN}",
    "Content-Type": "application/json"
}
# === JSONë§Œ í¬í•¨ëœ zipìœ¼ë¡œ ë®ì–´ì“°ê¸° ===
def extract_json_and_only_json(zip_path: Path):
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        json_files = [f for f in zip_ref.namelist() if f.endswith(".json") and f.startswith("annotations/")]
        if not json_files:
            print(f"âš ï¸ JSON íŒŒì¼ ì—†ìŒ: {zip_path}")
            return
        
        json_internal_path = json_files[0]
        json_data = zip_ref.read(json_internal_path)
        json_filename = zip_path.stem + ".json"

    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as new_zip:
        new_zip.writestr(json_filename, json_data)

    print(f"ğŸ“¦ JSONë§Œ í¬í•¨ëœ zipìœ¼ë¡œ ë®ì–´ì“°ê¸° ì™„ë£Œ: {zip_path.name}")

# === ìœ í‹¸ í•¨ìˆ˜ ===
def get_all_jobs():
    jobs = []
    page = 1
    while True:
        r = requests.get(f"{CVAT_URL}/api/jobs?page={page}", headers=HEADERS)
        r.raise_for_status()
        data = r.json()
        jobs.extend(data["results"])
        if not data["next"]:
            break
        page += 1
    return jobs

def get_task_info(task_id):
    r = requests.get(f"{CVAT_URL}/api/tasks/{task_id}", headers=HEADERS)
    r.raise_for_status()
    return r.json()

def get_organization_name(org_id):
    if not org_id:
        return "(None)"
    r = requests.get(f"{CVAT_URL}/api/organizations/{org_id}", headers=HEADERS)
    if r.status_code == 404:
        return "(Not found)"
    r.raise_for_status()
    return r.json().get("slug", f"(org-{org_id})")

def load_assignee_map_from_env():
    assignee_map = {}
    for key, value in os.environ.items():
        if key.startswith("USERMAP_"):
            username = key.replace("USERMAP_", "")
            assignee_map[username] = value
    return assignee_map

# === CLI Export ì‹¤í–‰ ===
def run_cvat_cli_export(task_id: int, task_name: str, assignee: str, result_dir: Path, export_log_path: Path, assignee_map: dict):
    safe_name = task_name.replace(" ", "_")
    output_path = result_dir / f"{safe_name}_boundingbox.zip" # boundingbox ë•Œë¬¸ì— ì¶”ê°€í•œê±° ë‚œì¤‘ì— ì‚­ì œí•´ì•¼í•¨
    exported_date = datetime.today().strftime("%Y-%m-%d")

    command = (
        f'cvat-cli --server-host {CVAT_URL} '
        f'--auth {CVAT_USERNAME}:{CVAT_PASSWORD} '
        f'task export-dataset {task_id} "{output_path}" '
        f'--format "{CVAT_EXPORT_FORMAT}" '
        f'--with-images {WITH_IMAGES}'
    )

    try:
        print(f"ğŸš€ Exporting: Task {task_id} â†’ {output_path.name}")
        subprocess.run(command, shell=True, check=True)
        print(f"âœ… Export ì„±ê³µ â†’ {output_path}")

        # âœ… assignee ë§¤í•‘ ë¡œë“œ
        assignee_map = load_assignee_map_from_env()

        mapped_assignee = assignee_map.get(assignee, assignee)

        # âœ… ë¡œê·¸ì— ê¸°ë¡ ë° ìƒíƒœ ë³€ê²½ CSV
        with open(export_log_path, "a", newline='') as f:
            writer = csv.writer(f)
            writer.writerow([task_id, task_name, mapped_assignee, exported_date])

        # âœ… JSONë§Œ í¬í•¨ëœ zipìœ¼ë¡œ ë®ì–´ì“°ê¸°
        extract_json_and_only_json(output_path)


    except subprocess.CalledProcessError as e:
        print(f"âŒ Export ì‹¤íŒ¨: Task {task_id} - {e}")

# === ë©”ì¸ ===
def main():
    jobs = get_all_jobs()

    # âœ… ê²°ê³¼ í´ë” ì„¤ì •
    today_str = datetime.today().strftime("%Y-%m-%d")
    result_dir = Path(f"{RESULT_DIR}/{today_str}")
    result_dir.mkdir(parents=True, exist_ok=True)

    export_log_path = Path("/home/pia/work_p/dfn/omission/result/export_log_2.csv")

    if not export_log_path.exists():
        with open(export_log_path, "w", newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["task_id", "task_name", "assignee", "exported_date"])

    # ê¸°âœ… CSV íŒŒì¼ ì½ê³  ì´ë¯¸ exportëœ task_id ì¶”ì¶œ
    exported_task_ids = set()
    with open(export_log_path, "r", newline='') as f:
        reader = csv.DictReader(f)
        for row in reader:
            exported_task_ids.add(row["task_id"])

    # âœ…ìœ ì € ë§¤í•‘ ë¶ˆëŸ¬ì˜¤ê¸°
    assignee_map = load_assignee_map_from_env()

    for job in jobs:
        task_id = str(job["task_id"])
        state = job.get("state")
        stage = job.get("stage")

        if not (stage == "acceptance" and state == "completed"):
            continue

        if task_id in exported_task_ids:
            print(f"â© ì´ë¯¸ exportë¨ â†’ Task {task_id}, ê±´ë„ˆëœ€")
            continue

        task_info = get_task_info(int(task_id))
        task_name = task_info.get("name", f"task_{task_id}")
        task_org_id = task_info.get("organization")
        task_org_slug = get_organization_name(task_org_id)

        if ORG_FILTER and task_org_slug != ORG_FILTER:
            continue

        assignee_info = job.get("assignee")
        assignee = assignee_info.get("username", "(unassigned)") if assignee_info else "(unassigned)"
        run_cvat_cli_export(int(task_id), task_name, assignee, result_dir, export_log_path, assignee_map)

if __name__ == "__main__":
    main()
